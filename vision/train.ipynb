{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# L2CS-Net의 train.py 코드 수정\n",
        "\n",
        "- Adapted from: https://github.com/Ahmednull/L2CS-Net/blob/main/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWI-jxTM_pDN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class L2CS_MobileNetV3(nn.Module):\n",
        "    def __init__(self, num_bins=90, version='large'):\n",
        "        super(L2CS_MobileNetV3, self).__init__()\n",
        "\n",
        "        if version == 'large':\n",
        "            self.feature_extractor = models.mobilenet_v3_large(pretrained=True)\n",
        "            feature_dim = 960\n",
        "        elif version == 'small':\n",
        "            self.feature_extractor = models.mobilenet_v3_small(pretrained=True)\n",
        "            feature_dim = 576\n",
        "        else:\n",
        "            raise ValueError(\"Invalid MobileNetV3 version. Choose 'large' or 'small'.\")\n",
        "\n",
        "        self.feature_extractor.classifier = nn.Identity()\n",
        "\n",
        "        self.fc_yaw_gaze = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_bins)\n",
        "        )\n",
        "\n",
        "        self.fc_pitch_gaze = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_bins)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        yaw = self.fc_yaw_gaze(features)\n",
        "        pitch = self.fc_pitch_gaze(features)\n",
        "        return pitch, yaw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbNuWuXv_5Re"
      },
      "outputs": [],
      "source": [
        "# === Parameter Grouping Functions ===\n",
        "def get_ignored_params(model):\n",
        "    # Ignore first conv layer & batchnorm for fine-tuning\n",
        "    b = [model.feature_extractor.features[0]]\n",
        "    for i in range(len(b)):\n",
        "        for module_name, module in b[i].named_modules():\n",
        "            if 'bn' in module_name:\n",
        "                module.eval()\n",
        "            for name, param in module.named_parameters():\n",
        "                yield param\n",
        "\n",
        "def get_non_ignored_params(model):\n",
        "    # Fine-tune deeper feature extractor layers\n",
        "    b = [model.feature_extractor.features[1:]]\n",
        "    for i in range(len(b)):\n",
        "        for module_name, module in b[i].named_modules():\n",
        "            if 'bn' in module_name:\n",
        "                module.eval()\n",
        "            for name, param in module.named_parameters():\n",
        "                yield param\n",
        "\n",
        "def get_fc_params(model):\n",
        "    # Train only final FC layers\n",
        "    b = [model.fc_yaw_gaze, model.fc_pitch_gaze]\n",
        "    for i in range(len(b)):\n",
        "        for module_name, module in b[i].named_modules():\n",
        "            for name, param in module.named_parameters():\n",
        "                yield param\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlNaezPT_9rP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image, ImageFilter\n",
        "\n",
        "\n",
        "class Gaze360(Dataset):\n",
        "    def __init__(self, path, root, transform, angle, binwidth, train=True):\n",
        "        self.transform = transform\n",
        "        self.root = root\n",
        "        self.orig_list_len = 0\n",
        "        self.angle = angle\n",
        "        if train==False:\n",
        "          angle=90\n",
        "        self.binwidth=binwidth\n",
        "        self.lines = []\n",
        "        if isinstance(path, list):\n",
        "            for i in path:\n",
        "                with open(i) as f:\n",
        "                    print(\"here\")\n",
        "                    line = f.readlines()\n",
        "                    line.pop(0)\n",
        "                    self.lines.extend(line)\n",
        "        else:\n",
        "            with open(path) as f:\n",
        "                lines = f.readlines()\n",
        "                lines.pop(0)\n",
        "                self.orig_list_len = len(lines)\n",
        "                for line in lines:\n",
        "                    gaze2d = line.strip().split(\" \")[5]\n",
        "                    label = np.array(gaze2d.split(\",\")).astype(\"float\")\n",
        "                    if abs((label[0]*180/np.pi)) <= angle and abs((label[1]*180/np.pi)) <= angle:\n",
        "                        self.lines.append(line)\n",
        "\n",
        "\n",
        "        print(\"{} items removed from dataset that have an angle > {}\".format(self.orig_list_len-len(self.lines), angle))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        line = self.lines[idx]\n",
        "        line = line.strip().split(\" \")\n",
        "\n",
        "        face = line[0]\n",
        "        lefteye = line[1]\n",
        "        righteye = line[2]\n",
        "        name = line[3]\n",
        "        gaze2d = line[5]\n",
        "        label = np.array(gaze2d.split(\",\")).astype(\"float\")\n",
        "        label = torch.from_numpy(label).type(torch.FloatTensor)\n",
        "\n",
        "        pitch = label[0]* 180 / np.pi\n",
        "        yaw = label[1]* 180 / np.pi\n",
        "\n",
        "        img = Image.open(os.path.join(self.root, face))\n",
        "\n",
        "        # fimg = cv2.imread(os.path.join(self.root, face))\n",
        "        # fimg = cv2.resize(fimg, (448, 448))/255.0\n",
        "        # fimg = fimg.transpose(2, 0, 1)\n",
        "        # img=torch.from_numpy(fimg).type(torch.FloatTensor)\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Bin values\n",
        "        bins = np.array(range(-1*self.angle, self.angle, self.binwidth))\n",
        "        binned_pose = np.digitize([pitch, yaw], bins) - 1\n",
        "\n",
        "        labels = binned_pose\n",
        "        cont_labels = torch.FloatTensor([pitch, yaw])\n",
        "\n",
        "\n",
        "        return img, labels, cont_labels, name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVUC1Ny1ACDM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import time\n",
        "import datetime\n",
        "import pytz\n",
        "\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_Gaze360(config):\n",
        "    cudnn.enabled = True\n",
        "    num_epochs = config[\"epochs\"]\n",
        "    lr = config['learning_rate']\n",
        "    batch_size = config['batch_size']\n",
        "    gpu = torch.device(\"cuda\")\n",
        "    data_set='gaze360'\n",
        "    alpha = 1\n",
        "    output='/content/drive/MyDrive/L2CSMobile/outputs/snapshots/'\n",
        "    arch = config['architecture']\n",
        "    snapshot = config['snapshot']\n",
        "    gaze360label_dir='/content/Gaze361/Label/train.label'\n",
        "    gaze360image_dir='/content/Gaze361/Image'\n",
        "\n",
        "\n",
        "\n",
        "    transformations = transforms.Compose([\n",
        "        transforms.Resize(448),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "\n",
        "\n",
        "    if data_set==\"gaze360\":\n",
        "      model = L2CS_MobileNetV3(90, version='large') # select model\n",
        "      model.to(gpu)\n",
        "\n",
        "      # Optimizer gaze\n",
        "      optimizer_gaze = torch.optim.Adam([\n",
        "          {'params': get_ignored_params(model), 'lr': 0},\n",
        "          {'params': get_non_ignored_params(model), 'lr': lr},\n",
        "          {'params': get_fc_params(model), 'lr': lr}\n",
        "      ], lr)\n",
        "\n",
        "      start_epoch = 0\n",
        "\n",
        "      if snapshot != '':\n",
        "          checkpoint = torch.load(snapshot)\n",
        "          model.load_state_dict(checkpoint['model_state_dict'])\n",
        "          optimizer_gaze.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "          start_epoch = checkpoint['epoch']\n",
        "\n",
        "      dataset=Gaze360(gaze360label_dir, gaze360image_dir, transformations, 180, 4)\n",
        "      print('Loading data.')\n",
        "      train_loader_gaze = DataLoader(\n",
        "          dataset=dataset,\n",
        "          batch_size=int(batch_size),\n",
        "          shuffle=True,\n",
        "          num_workers=0,\n",
        "          pin_memory=True)\n",
        "      torch.backends.cudnn.benchmark = True\n",
        "\n",
        "      timezone = pytz.timezone('Asia/Seoul')\n",
        "      seoul_time = datetime.datetime.now(timezone)\n",
        "      summary_name = '{}_{}'.format('L2CSM-gaze360-', seoul_time.strftime('%Y-%m-%d_%H-%M-%S'))\n",
        "      output=os.path.join(output, summary_name)\n",
        "      if not os.path.exists(output):\n",
        "          os.makedirs(output)\n",
        "\n",
        "\n",
        "      criterion = nn.CrossEntropyLoss().to(gpu)\n",
        "      reg_criterion = nn.MSELoss().to(gpu)\n",
        "      softmax = nn.Softmax(dim=1).to(gpu)\n",
        "      idx_tensor = [idx for idx in range(90)]\n",
        "      idx_tensor = Variable(torch.FloatTensor(idx_tensor)).to(gpu)\n",
        "\n",
        "\n",
        "\n",
        "      configuration = f\"\\ntrain configuration, batch_size={batch_size}, model_arch={arch}\\nStart testing dataset={data_set}, loader={len(train_loader_gaze)}------------------------- \\n\"\n",
        "      print(configuration)\n",
        "      # wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "      for epoch in range(start_epoch, num_epochs):\n",
        "          sum_loss_pitch_gaze = sum_loss_yaw_gaze = iter_gaze = 0\n",
        "\n",
        "          train_loader_gaze_tqdm = tqdm(train_loader_gaze, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "\n",
        "          for i, (images_gaze, labels_gaze, cont_labels_gaze,name) in enumerate(train_loader_gaze_tqdm):\n",
        "              images_gaze = Variable(images_gaze).to(gpu)\n",
        "\n",
        "              # Binned labels\n",
        "              label_pitch_gaze = Variable(labels_gaze[:, 0]).to(gpu)\n",
        "              label_yaw_gaze = Variable(labels_gaze[:, 1]).to(gpu)\n",
        "\n",
        "              # Continuous labels\n",
        "              label_pitch_cont_gaze = Variable(cont_labels_gaze[:, 0]).to(gpu)\n",
        "              label_yaw_cont_gaze = Variable(cont_labels_gaze[:, 1]).to(gpu)\n",
        "\n",
        "              pitch, yaw = model(images_gaze)\n",
        "\n",
        "              # Cross entropy loss\n",
        "              loss_pitch_gaze = criterion(pitch, label_pitch_gaze)\n",
        "              loss_yaw_gaze = criterion(yaw, label_yaw_gaze)\n",
        "\n",
        "              # MSE loss\n",
        "              pitch_predicted = softmax(pitch)\n",
        "              yaw_predicted = softmax(yaw)\n",
        "\n",
        "              pitch_predicted = \\\n",
        "                  torch.sum(pitch_predicted * idx_tensor, 1) * 4 - 180\n",
        "              yaw_predicted = \\\n",
        "                  torch.sum(yaw_predicted * idx_tensor, 1) * 4 - 180\n",
        "\n",
        "              loss_reg_pitch = reg_criterion(\n",
        "                  pitch_predicted, label_pitch_cont_gaze)\n",
        "              loss_reg_yaw = reg_criterion(\n",
        "                  yaw_predicted, label_yaw_cont_gaze)\n",
        "\n",
        "              # Total loss\n",
        "              loss_pitch_gaze += alpha * loss_reg_pitch\n",
        "              loss_yaw_gaze += alpha * loss_reg_yaw\n",
        "\n",
        "              sum_loss_pitch_gaze += loss_pitch_gaze\n",
        "              sum_loss_yaw_gaze += loss_yaw_gaze\n",
        "\n",
        "              loss_seq = [loss_pitch_gaze, loss_yaw_gaze]\n",
        "              grad_seq = [torch.tensor(1.0).to(gpu) for _ in range(len(loss_seq))]\n",
        "              optimizer_gaze.zero_grad(set_to_none=True)\n",
        "              torch.autograd.backward(loss_seq, grad_seq)\n",
        "              optimizer_gaze.step()\n",
        "              # scheduler.step()\n",
        "\n",
        "              iter_gaze += 1\n",
        "\n",
        "              train_loader_gaze_tqdm.set_postfix({\n",
        "                  \"Loss Pitch\": sum_loss_pitch_gaze / iter_gaze,\n",
        "                  \"Loss Yaw\": sum_loss_yaw_gaze / iter_gaze\n",
        "              })\n",
        "\n",
        "              # if (i+1) % 100 == 0:\n",
        "              #     print('Epoch [%d/%d], Iter [%d/%d] Losses: '\n",
        "              #         'Gaze Yaw %.4f,Gaze Pitch %.4f' % (\n",
        "              #             epoch+1,\n",
        "              #             num_epochs,\n",
        "              #             i+1,\n",
        "              #             len(dataset)//batch_size,\n",
        "              #             sum_loss_pitch_gaze/iter_gaze,\n",
        "              #             sum_loss_yaw_gaze/iter_gaze\n",
        "              #         )\n",
        "              #         )\n",
        "\n",
        "\n",
        "          if epoch % 1 == 0 and epoch < num_epochs:\n",
        "              print('Taking snapshot...',\n",
        "                    torch.save({\n",
        "                        'epoch': epoch,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer_gaze.state_dict(),\n",
        "                    }, output + '/checkpoint_epoch_{}.pth'.format(epoch)))\n",
        "              wandb.log({\n",
        "                      \"loss_pitch_gaze\": sum_loss_pitch_gaze/iter_gaze,\n",
        "                      \"loss_yaw_gaze\": sum_loss_yaw_gaze/iter_gaze,}\n",
        "                      ,step=epoch+1\n",
        "                  )\n",
        "              wandb.save(output + '/checkpoint_epoch_{}.pth'.format(epoch))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBTPEGPUA-xl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!unzip -u /content/drive/MyDrive/L2CS-Net/Gaze360.zip -d /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "F-E12yd1A1ez",
        "outputId": "1a4cb888-2350-48d7-9a58-7c2c3806cae5"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R5RfpRBAHfm"
      },
      "outputs": [],
      "source": [
        "def run():\n",
        "    config  = {\n",
        "      'epochs': 50,\n",
        "      'batch_size': 16,\n",
        "      'learning_rate': 1e-5,\n",
        "      'dataset': 'Gaze360',\n",
        "      'architecture': 'MobileNetV3',\n",
        "      'optimizer': 'Adam',\n",
        "      'snapshot': ''\n",
        "    }\n",
        "    wandb.init(project='Itda-L2CS',name='MobilenetV3L')\n",
        "    # wandb.init(project='Itda-L2CS',name='MobilenetV3L', config=config, id=\"\", resume=\"must\")\n",
        "    train_Gaze360(config)\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tho6-CRBGZQM",
        "outputId": "e82109d3-1e97-4715-8b36-47ef9827a90e"
      },
      "outputs": [],
      "source": [
        "run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
