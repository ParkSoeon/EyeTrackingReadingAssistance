{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "few codes refers to 'https://blog.roboflow.com/gaze-direction-position/#bonus-using-gaze-points'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install inference supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import get_model\n",
    "\n",
    "# Use YOLOv8n\n",
    "model = get_model(model_id = \"yolov8n-640\") \n",
    "'''\n",
    "만약 괜찮다면 v8n 말고 다른 애들 중 적은 용량의 모델(Implement가 쉬울 것이기에) && 객체를 잘 탐지하는 모델을 또 실험해봐야될 것으로 예상\n",
    "+ 고려해야될 것들:\n",
    "- 공부 환경\n",
    "  * 조명에 영향을 많이 받는 환경일 것 -> Color Correction? 필요할 것\n",
    "- 정확도 및 민감도 조정:\n",
    "  * 특정 객체를 얼마나 잘 탐지하는지\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize Mediapipe FaceMesh\n",
    "'''Mediapipe FaceMesh는 얼굴의 좌표를 보다 정확히 알 수 있게 해줌'''\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Open the Camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "# Screen resolution for mapping\n",
    "screen_width, screen_height = 1920, 1080\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Preprocess the frame for Mediapipe\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    \n",
    "    '''편의상 시연할 때 거울과 같이 얼굴을 마주보고자 좌우반전을 함'''\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "        RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "        for eye_landmarks, eye_name in zip([LEFT_EYE, RIGHT_EYE], [\"Left Eye\", \"Right Eye\"]):\n",
    "            eye_points = np.array([\n",
    "                [int(face_landmarks.landmark[idx].x * frame.shape[1]),\n",
    "                 int(face_landmarks.landmark[idx].y * frame.shape[0])]\n",
    "                for idx in eye_landmarks\n",
    "            ])\n",
    "\n",
    "            # Draw eye landmarks\n",
    "            for point in eye_points:\n",
    "                cv2.circle(frame, tuple(point), 2, (0, 255, 0), -1)\n",
    "\n",
    "            # Eye region\n",
    "            x, y, w, h = cv2.boundingRect(eye_points)\n",
    "            roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "            # Process the eye region\n",
    "            gray_eye = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            blurred_eye = cv2.GaussianBlur(gray_eye, (5, 5), 0)\n",
    "            binary_eye = cv2.adaptiveThreshold(\n",
    "                blurred_eye, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                cv2.THRESH_BINARY_INV, 11, 2\n",
    "            )\n",
    "\n",
    "            contours, _ = cv2.findContours(binary_eye, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if contours:\n",
    "                largest_contour = max(contours, key=cv2.contourArea)\n",
    "                (cx, cy), radius = cv2.minEnclosingCircle(largest_contour)\n",
    "                cx, cy = int(cx), int(cy)\n",
    "\n",
    "                # Pupil position in the ROI\n",
    "                cv2.circle(roi, (cx, cy), int(radius), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, f\"{eye_name} Pupil: ({cx},{cy})\", (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "                # Map pupil to screen coordinates\n",
    "                screen_x = int(screen_width * (cx / w))\n",
    "                screen_y = int(screen_height * (cy / h))\n",
    "\n",
    "                # Display screen mapping\n",
    "                cv2.circle(frame, (screen_x, screen_y), 10, (0, 255, 255), -1)\n",
    "                cv2.putText(frame, f\"Screen: ({screen_x},{screen_y})\", (x, y + h + 20),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    # Convert back to BGR for display\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Eye Tracking\", frame)\n",
    "\n",
    "    # Exit condition\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
